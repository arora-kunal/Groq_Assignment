{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAE/ZEn2BpECJDHjTcjZxk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arora-kunal/Groq_Assignment/blob/main/Groq_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KGlXDczmTTwN",
        "outputId": "9d698680-b8b6-447b-8dd1-70fe4c428831"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.106.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (4.25.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema) (0.27.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai jsonschema"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from openai import OpenAI\n",
        "import jsonschema\n",
        "from jsonschema import validate\n",
        "\n",
        "from google.colab import userdata\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "# Initialize OpenAI client with Groq base URL\n",
        "client = OpenAI(\n",
        "    api_key=GROQ_API_KEY,\n",
        "    base_url=\"https://api.groq.com/openai/v1\"\n",
        ")\n",
        "\n",
        "MODEL = \"llama-3.1-8b-instant\""
      ],
      "metadata": {
        "id": "VGaUZ0uoTdI1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Chunk 2: Conversation Functions ===\n",
        "\n",
        "# Truncate history by keeping max_turns of user+assistant pairs\n",
        "def truncate_history(history, max_turns=None):\n",
        "    if max_turns:\n",
        "        user_assistant_msgs = [m for m in history if m[\"role\"] in (\"user\", \"assistant\")]\n",
        "        if len(user_assistant_msgs) > max_turns * 2:\n",
        "            keep = user_assistant_msgs[-max_turns*2:]\n",
        "            history = [m for m in history if m[\"role\"] == \"system\"] + keep\n",
        "    return history\n",
        "\n",
        "# Summarize history into a concise form\n",
        "def summarize_history(history):\n",
        "    conversation = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in history])\n",
        "    prompt = (\n",
        "        f\"Summarize this conversation briefly, keeping important details:\\n\\n{conversation}\"\n",
        "    )[:4000]  # keep prompt short\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=150\n",
        "    )\n",
        "\n",
        "    msg = response.choices[0].message\n",
        "    return msg[\"content\"].strip() if isinstance(msg, dict) else msg.content.strip()\n",
        "\n",
        "# Chat function with memory\n",
        "def chat(history, user_input, memory=\"truncate\", max_turns=5):\n",
        "    history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    if memory == \"truncate\":\n",
        "        history = truncate_history(history, max_turns=max_turns)\n",
        "    elif memory == \"summarize\" and len(history) > max_turns * 2:\n",
        "        summary = summarize_history(history)\n",
        "        history = [{\"role\": \"system\", \"content\": f\"Summary of past conversation: {summary}\"}] + history[-2:]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=history,\n",
        "        max_tokens=200\n",
        "    )\n",
        "\n",
        "    msg = response.choices[0].message\n",
        "    assistant_reply = msg[\"content\"].strip() if isinstance(msg, dict) else msg.content.strip()\n",
        "    history.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
        "\n",
        "    return assistant_reply, history\n"
      ],
      "metadata": {
        "id": "iQq1K2CRUdpU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimate tokens (roughly words * 1.3)\n",
        "def estimate_tokens(text):\n",
        "    if not text or text.isspace():\n",
        "        return 0\n",
        "    return int(len(text.split()) * 1.3) + 1\n",
        "\n",
        "# Truncate history by turns or tokens\n",
        "def truncate_history(history, max_turns=None, max_tokens=None):\n",
        "    if not history:\n",
        "        return []\n",
        "    if max_turns:\n",
        "        return history[-max_turns:]\n",
        "    if max_tokens:\n",
        "        truncated = []\n",
        "        current_tokens = 0\n",
        "        for msg in reversed(history):\n",
        "            msg_tokens = estimate_tokens(msg['content'])\n",
        "            if current_tokens + msg_tokens > max_tokens:\n",
        "                break\n",
        "            truncated.insert(0, msg)\n",
        "            current_tokens += msg_tokens\n",
        "        return truncated\n",
        "    return history\n",
        "\n",
        "# Summarize history using Groq API\n",
        "def summarize_history(history):\n",
        "    if not history:\n",
        "        return \"No conversation to summarize.\"\n",
        "    try:\n",
        "        prompt = \"Summarize this conversation concisely (≤100 words). Preserve key facts and intents.\\n\\n\"\n",
        "        for msg in history:\n",
        "            prompt += f\"{msg['role'].capitalize()}: {msg['content']}\\n\"\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a precise summarizer.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=150\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error in summarization: {e}\")\n",
        "        return \"Summary failed.\""
      ],
      "metadata": {
        "id": "7fovI854U0Qq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConversationManager:\n",
        "    def __init__(self, max_turns=None, max_tokens=None, summarize_every_k=None, keep_last_n=4):\n",
        "        self.history = []\n",
        "        self.max_turns = max_turns\n",
        "        self.max_tokens = max_tokens\n",
        "        self.summarize_every_k = summarize_every_k\n",
        "        self.keep_last_n = keep_last_n  # Keep last N exchanges along with summary\n",
        "        self.user_turn_count = 0\n",
        "\n",
        "    def add_message(self, role, content):\n",
        "        self.history.append({\"role\": role, \"content\": content})\n",
        "        if role == \"user\":\n",
        "            self.user_turn_count += 1\n",
        "        self._manage_history()\n",
        "\n",
        "    def _manage_history(self):\n",
        "        # Apply truncation first\n",
        "        self.history = truncate_history(self.history, self.max_turns, self.max_tokens)\n",
        "\n",
        "        # Periodic summarization\n",
        "        if (self.summarize_every_k and\n",
        "            self.user_turn_count % self.summarize_every_k == 0):\n",
        "\n",
        "            summary = summarize_history(self.history)\n",
        "\n",
        "            # Keep summary + last N exchanges\n",
        "            new_history = [{\"role\": \"system\", \"content\": f\"Here’s a summary of what we talked about earlier: {summary}\"}]\n",
        "            recent_msgs = [msg for msg in self.history if msg['role'] != \"system\"][-self.keep_last_n:]\n",
        "            new_history.extend(recent_msgs)\n",
        "\n",
        "            self.history = truncate_history(new_history, self.max_turns, self.max_tokens)\n",
        "\n",
        "    def get_history(self):\n",
        "        return self.history\n",
        "\n",
        "    def chat(self, user_input):\n",
        "        self.add_message(\"user\", user_input)\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=MODEL,\n",
        "                messages=self.history,\n",
        "                max_tokens=300,\n",
        "                temperature=0.7\n",
        "            )\n",
        "            assistant_reply = response.choices[0].message.content.strip()\n",
        "            self.add_message(\"assistant\", assistant_reply)\n",
        "            return assistant_reply\n",
        "        except Exception as e:\n",
        "            print(f\"Error in chat: {e}\")\n",
        "            return \"Response failed.\""
      ],
      "metadata": {
        "id": "VMS23W7lfy3r"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Demo 1: Truncate Memory ===\")\n",
        "manager1 = ConversationManager(max_turns=8)\n",
        "for i in range(6):\n",
        "    user_msg = f\"This is message {i+1}\"\n",
        "    resp = manager1.chat(user_msg)\n",
        "    print(f\"User: {user_msg}\")\n",
        "    print(f\"Assistant: {resp}\")\n",
        "    print(f\"History length: {len(manager1.get_history())}\\n\")\n",
        "\n",
        "print(\"=== Demo 2: Summarize Memory ===\")\n",
        "manager2 = ConversationManager(summarize_every_k=3, keep_last_n=4)\n",
        "for i in range(6):\n",
        "    user_msg = f\"This is message {i+1}\"\n",
        "    resp = manager2.chat(user_msg)\n",
        "    print(f\"User: {user_msg}\")\n",
        "    print(f\"Assistant: {resp}\")\n",
        "    print(f\"History length: {len(manager2.get_history())}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrM3TmJCf-z0",
        "outputId": "c22cfa63-d2ed-49d4-d211-fcfef382d717"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Demo 1: Truncate Memory ===\n",
            "User: This is message 1\n",
            "Assistant: This is message 2. What would you like to talk about?\n",
            "History length: 2\n",
            "\n",
            "User: This is message 2\n",
            "Assistant: This is message 3. I see we're keeping track of our conversation. What's on your mind?\n",
            "History length: 4\n",
            "\n",
            "User: This is message 3\n",
            "Assistant: This is message 4. It seems we've got a pattern going on. Is there a particular topic you'd like to discuss, or would you like to keep the conversation light?\n",
            "History length: 6\n",
            "\n",
            "User: This is message 4\n",
            "Assistant: This is message 5. We're really getting into a rhythm now. Would you like to try a different topic or continue this exchange?\n",
            "History length: 8\n",
            "\n",
            "User: This is message 5\n",
            "Assistant: This is message 6. We've reached the sixth message. If you'd like, I can suggest some conversation topics or we can keep going with the message-counting game.\n",
            "History length: 8\n",
            "\n",
            "User: This is message 6\n",
            "Assistant: This is message 7. Our conversation is still going strong. How about we try something different? What are you interested in - a particular hobby, a scientific topic, or something else?\n",
            "History length: 8\n",
            "\n",
            "=== Demo 2: Summarize Memory ===\n",
            "User: This is message 1\n",
            "Assistant: This is message 2. What would you like to talk about?\n",
            "History length: 2\n",
            "\n",
            "User: This is message 2\n",
            "Assistant: This is message 3. It seems we're keeping track of the conversation. Would you like to start a topic or continue with something else?\n",
            "History length: 4\n",
            "\n",
            "User: This is message 3\n",
            "Assistant: It seems we have a loop going on. This is actually message 4. You initially sent a message (1), I responded (2), you repeated your message (3), and now you're repeating again (4). What's the actual topic you'd like to discuss?\n",
            "History length: 5\n",
            "\n",
            "User: This is message 4\n",
            "Assistant: It appears we're stuck in a repetition loop. To break the cycle, I'd like to suggest starting a new topic. What's something you're interested in or would like to talk about?\n",
            "History length: 7\n",
            "\n",
            "User: This is message 5\n",
            "Assistant: Let's start fresh. This is indeed a new message (5), not a repetition. Since we've broken the loop, I'll reset our conversation count. We can start discussing something new from here. What's on your mind?\n",
            "History length: 9\n",
            "\n",
            "User: This is message 6\n",
            "Assistant: It seems you're confirming the count. This is indeed message 6. I'll keep track of our conversation correctly from now on. What would you like to talk about?\n",
            "History length: 5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import jsonschema\n",
        "from jsonschema import validate\n",
        "import time  # Added to fix NameError\n",
        "\n",
        "# JSON Schema Definition (unchanged, allows nullable phone)\n",
        "EXTRACTION_SCHEMA = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"name\": {\"type\": \"string\", \"description\": \"Full name of the person.\"},\n",
        "        \"email\": {\"type\": \"string\", \"description\": \"Email address.\"},\n",
        "        \"phone\": {\n",
        "            \"type\": [\"string\", \"null\"],  # Allow null for missing phone\n",
        "            \"description\": \"Phone number, or null if not provided.\"\n",
        "        },\n",
        "        \"location\": {\"type\": \"string\", \"description\": \"City or full location.\"},\n",
        "        \"age\": {\"type\": \"integer\", \"description\": \"Age in years.\"}\n",
        "    },\n",
        "    \"required\": [\"name\", \"email\", \"location\", \"age\"],  # Phone not required\n",
        "    \"additionalProperties\": False\n",
        "}\n",
        "\n",
        "# Tool Definition for Function Calling\n",
        "TOOLS = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"extract_info\",\n",
        "            \"description\": (\n",
        "                \"Extract personal details (name, email, phone, location, age) from the chat text. \"\n",
        "                \"If a field (e.g., phone) is missing, return null for it. \"\n",
        "                \"Extract information even if it’s provided in an assistant’s response in a dialogue.\"\n",
        "            ),\n",
        "            \"parameters\": EXTRACTION_SCHEMA\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "# Function: Extract info using Groq/OpenAI function calling\n",
        "def extract_info_from_chat(chat_text, retries=3):\n",
        "    \"\"\"Use function calling to extract structured info, handling missing fields and errors.\"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=MODEL,\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": (\n",
        "                            \"You are an assistant that extracts structured personal information from chat text. \"\n",
        "                            \"Always use the 'extract_info' function to return name, email, phone, location, and age. \"\n",
        "                            \"If a field (e.g., phone) is missing, return null for it. \"\n",
        "                            \"Extract information from both user and assistant messages in dialogues. \"\n",
        "                            \"Ensure the output strictly matches the provided JSON schema.\"\n",
        "                        )\n",
        "                    },\n",
        "                    {\"role\": \"user\", \"content\": chat_text}\n",
        "                ],\n",
        "                tools=TOOLS,\n",
        "                tool_choice={\"type\": \"function\", \"function\": {\"name\": \"extract_info\"}},  # Force function use\n",
        "                max_tokens=300,\n",
        "                temperature=0.7  # Reduce hallucinations\n",
        "            )\n",
        "            # Check if tool_calls exists\n",
        "            if response.choices[0].message.tool_calls is None:\n",
        "                print(f\"Attempt {attempt+1} failed: No tool_calls in response.\")\n",
        "                if attempt < retries - 1:\n",
        "                    time.sleep(1)\n",
        "                continue\n",
        "            tool_call = response.choices[0].message.tool_calls[0]\n",
        "            extracted = json.loads(tool_call.function.arguments)\n",
        "            return extracted\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt+1} failed: {str(e)}\")\n",
        "            if attempt < retries - 1:\n",
        "                time.sleep(1)  # Wait before retrying\n",
        "    # Fallback if all retries fail\n",
        "    print(\"All retries failed. Returning None.\")\n",
        "    return None\n",
        "\n",
        "# Function: Validate extracted JSON\n",
        "def validate_extraction(extracted):\n",
        "    \"\"\"Validate against schema.\"\"\"\n",
        "    try:\n",
        "        validate(instance=extracted, schema=EXTRACTION_SCHEMA)\n",
        "        return True, \"Valid\"\n",
        "    except jsonschema.exceptions.ValidationError as e:\n",
        "        return False, str(e)"
      ],
      "metadata": {
        "id": "kGAI3iGbazqJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstration for Task 2\n",
        "# Use the provided sample chats\n",
        "sample_chats = [\n",
        "    \"Hey, I'm Alice Johnson, 28 years old, from Chicago. My email is alice.johnson@gmail.com, and my phone is 312-555-1234.\",\n",
        "    \"Hi, my name is Raj Patel, I'm 35, living in Mumbai. My email is raj.patel@outlook.com. I don't usually share my phone number.\",\n",
        "    \"User: Introduce yourself! Assistant: I'm just an AI, but you're Sarah Lee, right? I heard you're 22, from Sydney, with email sarah.lee@yahoo.com and phone +61-2-5550-9876.\"\n",
        "]\n",
        "\n",
        "# Demonstrate\n",
        "for i, chat in enumerate(sample_chats, 1):\n",
        "    print(f\"Sample Chat {i}: {chat}\")\n",
        "    extracted = extract_info_from_chat(chat)\n",
        "    if extracted:\n",
        "        print(\"Extracted JSON:\", json.dumps(extracted, indent=2))\n",
        "        is_valid, msg = validate_extraction(extracted)\n",
        "        print(f\"Validation: {is_valid} - {msg}\")\n",
        "    else:\n",
        "        print(\"Extraction failed after retries.\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oFZHSobhvLR",
        "outputId": "4843be89-b3ee-467d-b39c-751a3a76ab0a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Chat 1: Hey, I'm Alice Johnson, 28 years old, from Chicago. My email is alice.johnson@gmail.com, and my phone is 312-555-1234.\n",
            "Extracted JSON: {\n",
            "  \"age\": 28,\n",
            "  \"email\": \"alice.johnson@gmail.com\",\n",
            "  \"location\": \"Chicago\",\n",
            "  \"name\": \"Alice Johnson\",\n",
            "  \"phone\": \"312-555-1234\"\n",
            "}\n",
            "Validation: True - Valid\n",
            "\n",
            "\n",
            "Sample Chat 2: Hi, my name is Raj Patel, I'm 35, living in Mumbai. My email is raj.patel@outlook.com. I don't usually share my phone number.\n",
            "Extracted JSON: {\n",
            "  \"age\": 35,\n",
            "  \"email\": \"raj.patel@outlook.com\",\n",
            "  \"location\": \"Mumbai\",\n",
            "  \"name\": \"Raj Patel\",\n",
            "  \"phone\": null\n",
            "}\n",
            "Validation: True - Valid\n",
            "\n",
            "\n",
            "Sample Chat 3: User: Introduce yourself! Assistant: I'm just an AI, but you're Sarah Lee, right? I heard you're 22, from Sydney, with email sarah.lee@yahoo.com and phone +61-2-5550-9876.\n",
            "Extracted JSON: {\n",
            "  \"age\": 22,\n",
            "  \"email\": \"sarah.lee@yahoo.com\",\n",
            "  \"location\": \"Sydney\",\n",
            "  \"name\": \"Sarah Lee\",\n",
            "  \"phone\": \"+61-2-5550-9876\"\n",
            "}\n",
            "Validation: True - Valid\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OmTVd1DLiVeW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}